import { TypedEventTarget } from 'typescript-event-target';

interface AddTranscript {
    message: AddTranscriptMessageEnum;
    format?: string;
    metadata: RecognitionMetadata;
    results: Array<RecognitionResult>;
}
declare const AddTranscriptMessageEnum: {
    readonly AddTranscript: "AddTranscript";
};
type AddTranscriptMessageEnum = (typeof AddTranscriptMessageEnum)[keyof typeof AddTranscriptMessageEnum];

interface RecognitionAlternative {
    content: string;
    confidence: number;
    language?: string;
    display?: RecognitionDisplay;
    speaker?: string;
}

interface RecognitionResult {
    type: RecognitionResultTypeEnum;
    start_time: number;
    end_time: number;
    channel?: string;
    attaches_to?: RecognitionResultAttachesToEnum;
    is_eos?: boolean;
    alternatives?: Array<RecognitionAlternative>;
    score?: number;
    volume?: number;
}
declare const RecognitionResultTypeEnum: {
    readonly Word: "word";
    readonly Punctuation: "punctuation";
};
type RecognitionResultTypeEnum = (typeof RecognitionResultTypeEnum)[keyof typeof RecognitionResultTypeEnum];
declare const RecognitionResultAttachesToEnum: {
    readonly Next: "next";
    readonly Previous: "previous";
    readonly None: "none";
    readonly Both: "both";
};
type RecognitionResultAttachesToEnum = (typeof RecognitionResultAttachesToEnum)[keyof typeof RecognitionResultAttachesToEnum];

interface RecognitionMetadata {
    start_time: number;
    end_time: number;
    transcript: string;
}

interface RecognitionDisplay {
    direction: RecognitionDisplayDirectionEnum;
}
declare const RecognitionDisplayDirectionEnum: {
    readonly Ltr: "ltr";
    readonly Rtl: "rtl";
};
type RecognitionDisplayDirectionEnum = (typeof RecognitionDisplayDirectionEnum)[keyof typeof RecognitionDisplayDirectionEnum];

type ConversationStartedMessage = {
    message: 'ConversationStarted';
    id: string;
    asr_session_id: string;
    language_pack_info: {
        adapted: boolean;
        itn: boolean;
        language_description: string;
        word_delimiter: string;
        writing_direction: 'left-to-right' | 'right-to-left';
    };
};
type AudioAddedMessage = {
    message: 'AudioAdded';
    seq_no: number;
};
type ResponseStartedMessage = {
    message: 'ResponseStarted';
    content: string;
    start_time: number;
};
type ResponseCompletedMessage = {
    message: 'ResponseCompleted';
    content: string;
    start_time: number;
    end_time: number;
};
type AddTranscriptMessage = {
    message: 'AddTranscript';
    results: Array<RecognitionResult>;
    metadata: RecognitionMetadata;
};
interface AddPartialTranscriptMessage {
    message: 'AddPartialTranscript';
    format?: string;
    metadata: RecognitionMetadata;
    results: Array<RecognitionResult>;
}
type ResponseInterruptedMessage = {
    message: 'ResponseInterrupted';
    content: string;
    start_time: number;
    end_time: number;
};
type ConversationEndingMessage = {
    message: 'ConversationEnding';
};
type ConversationEndedMessage = {
    message: 'ConversationEnded';
};
type InfoMessage = {
    message: 'Info';
    [k: string]: unknown;
};
type WarningMessage = {
    message: 'Warning';
    [k: string]: unknown;
};
type ErrorMessage = {
    message: 'Error';
    [k: string]: unknown;
};
type FlowClientIncomingMessage = ConversationStartedMessage | AudioAddedMessage | ResponseStartedMessage | ResponseCompletedMessage | ResponseInterruptedMessage | AddTranscriptMessage | AddPartialTranscriptMessage | ConversationEndingMessage | ConversationEndedMessage | InfoMessage | WarningMessage | ErrorMessage;
type AudioFormat = {
    type: 'raw';
    encoding: 'pcm_s16le' | 'pcm_f32le';
    sample_rate: number;
};
interface StartConversationMessage {
    message: 'StartConversation';
    conversation_config: {
        template_id: string;
        template_variables: {
            [key: string]: string;
        };
    };
    audio_format: AudioFormat;
}
interface AudioReceivedMessage {
    message: 'AudioReceived';
    seq_no: number;
    buffering: number;
}
interface AudioEndedMessage {
    message: 'AudioEnded';
    last_seq_no: number;
}
type FlowClientOutgoingMessage = StartConversationMessage | AudioReceivedMessage | AudioEndedMessage;
declare class AgentAudioEvent extends Event {
    readonly data: Int16Array;
    constructor(data: Int16Array);
}
declare class FlowIncomingMessageEvent extends Event {
    readonly data: FlowClientIncomingMessage;
    constructor(data: FlowClientIncomingMessage);
}
interface FlowClientEventMap {
    agentAudio: AgentAudioEvent;
    message: FlowIncomingMessageEvent;
    socketInitialized: Event;
    socketOpen: Event;
    socketClosing: Event;
    socketClose: Event;
    socketError: Event;
}

interface FlowClientOptions {
    appId: string;
    audioBufferingMs?: number;
    websocketBinaryType?: WebSocket['binaryType'];
}
declare class FlowClient extends TypedEventTarget<FlowClientEventMap> {
    readonly server: string;
    readonly appId: string;
    private readonly audioBufferingMs;
    private readonly websocketBinaryType;
    constructor(server: string, { appId, audioBufferingMs, websocketBinaryType, }: FlowClientOptions);
    private ws;
    private serverSeqNo;
    private clientSeqNo;
    private jitterBuffer;
    get socketState(): "connecting" | "open" | "closing" | "closed" | undefined;
    private connect;
    private setupSocketEventListeners;
    private handleWebsocketAudio;
    private handleWebsocketMessage;
    private sendWebsocketMessage;
    sendAudio(pcmData: ArrayBufferLike): void;
    startConversation(jwt: string, { config, audioFormat, }: {
        config: StartConversationMessage['conversation_config'];
        audioFormat?: StartConversationMessage['audio_format'];
    }): Promise<void>;
    endConversation(): void;
    private disconnectSocket;
}
type SpeechmaticsFlowErrorType = 'SocketNotClosed' | 'SocketClosedPrematurely' | 'SocketError' | 'ServerError' | 'UnexpectedMessage' | 'BadBinaryMessage' | 'Timeout';
declare class SpeechmaticsFlowError extends Error {
    readonly type: SpeechmaticsFlowErrorType;
    constructor(type: SpeechmaticsFlowErrorType, message: string, options?: ErrorOptions);
}

interface Personas {
    [personaId: string]: {
        name: string;
        character: string;
        description: string;
        start_text: string[];
        avatar?: string;
    };
}
declare function fetchPersonas(): Promise<Personas>;

export { type AddPartialTranscriptMessage, type AddTranscript, type AddTranscriptMessage, AddTranscriptMessageEnum, AgentAudioEvent, type AudioAddedMessage, type AudioEndedMessage, type AudioReceivedMessage, type ConversationEndedMessage, type ConversationEndingMessage, type ConversationStartedMessage, type ErrorMessage, FlowClient, type FlowClientEventMap, type FlowClientIncomingMessage, type FlowClientOptions, type FlowClientOutgoingMessage, FlowIncomingMessageEvent, type InfoMessage, type RecognitionAlternative, type RecognitionDisplay, RecognitionDisplayDirectionEnum, type RecognitionMetadata, type RecognitionResult, RecognitionResultAttachesToEnum, RecognitionResultTypeEnum, type ResponseCompletedMessage, type ResponseInterruptedMessage, type ResponseStartedMessage, SpeechmaticsFlowError, type SpeechmaticsFlowErrorType, type StartConversationMessage, type WarningMessage, fetchPersonas };
